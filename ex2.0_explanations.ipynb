{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0b6017",
   "metadata": {},
   "source": [
    "# Tutorial 2: Conditional Generative Adversarial Network (CGAN)\n",
    "## Generating visible from infra-red satellite images\n",
    "\n",
    "**Authors:** Paula Bigalke, Claudia Acquistapace, Daniele Corradini\n",
    "\n",
    "**Affiliation:** Institute of Geophysics and Meteorology, University of Cologne, Germany\n",
    "\n",
    "**Contact**: paula.bigalke@uni-koeln.de, cacquist@uni-koeln.de, dcorrad1@uni-koeln.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-irish",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn how to implement and train a CGAN model with satellite images. CGANs consist of two models working against each other: the generator and the discriminator. The generator tries to create fake images. At the same time the discriminator tries to distinguish between these fake and the real images. Both want to win the battle! The generator aims at creating such realistic fake images that it can fool the discriminator into believing them to be real. And the discriminator wants to be so clever that he can always recognize the fake images. We will end up with a well trained generator of fake satellite images.\n",
    "\n",
    "But wait a second - what do we need fake satellite images for? We have plenty of real ones! The thing is, images from the visible channels are only available during the day, where the sensor measures the sunlight reflected by the surface and cloud tops. Infra-red channels, on the other hand, are producing images at every time - also during the night! What if the generator could learn to recreate the visible (VIS) images from the corresponding infra-red (IR) images? Then we could generate our own VIS images also during the night and have a more complete view of the clouds and atmosphere! This kind of task to create an unknown image from a known image is called image-to-image-translation. \n",
    "\n",
    "Let's try it!\n",
    "\n",
    "This tutorial will guide you through the different steps needed for running a CGAN model. Mainly it consists of 3 parts (notebooks):\n",
    "\n",
    "**1. Dataset preparation and pre-processing** (ex2.1_data_preparation.ipynb)\n",
    "\n",
    "**2. Setting up the model** (ex2.2_setting_up_model.ipynb)\n",
    "\n",
    "**3. Training and evaluation of the model** (ex2.3_training_network.ipynb)\n",
    "\n",
    "You will be asked to fill in some parts of the code marked with **#### TODO ####** and answer some comprehension questions. Please, provide your answer right below the respective question. For each task and question you will be given points. In total there are **x points**. An overview of the whole tutorial is given below.\n",
    "\n",
    "Have fun!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9f7bf",
   "metadata": {},
   "source": [
    "### 2.0 Setting up the Notebook\n",
    "\n",
    "**Upload material to your own drive:** Go to this link and download all the material: **INSERT LINK HERE**. Upload everything to your own drive. Then open the notebooks\n",
    "\n",
    "\n",
    "**Access permission:**\n",
    "Then, you need to give Google Collab the permission to use your Drive (a window will pop up). And you need to specify the path to your personal Drive folder where you saved this exercise to.\n",
    "\n",
    "**Adjusting path:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d34db7",
   "metadata": {},
   "source": [
    "### 2.1 Dataset preparation and pre-processing (16 / y points)\n",
    "\n",
    "    (a) Train and test dataset (2 p)\n",
    "\n",
    "    (b) Load and plot example images (4 p)\n",
    "\n",
    "    (c) Normalization (3 p)\n",
    "\n",
    "    (d) Data augmentation (3 p)\n",
    "\n",
    "    (e) Loading in the complete datasets (4 p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3387e3",
   "metadata": {},
   "source": [
    "### 2.2 Setting up the model (3 / y points)\n",
    "\n",
    "    (a) Generator (2 p)\n",
    "\n",
    "    (b) Discriminator (1 p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7859c89",
   "metadata": {},
   "source": [
    "### 2.3 Training and evaluation of the model (11 / 30 points)\n",
    "\n",
    "    (a) Monitoring the training progress (3 p)\n",
    "\n",
    "    (b) Training the network (4 p)\n",
    "    \n",
    "    (c) Interpretation of losses (4 p)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e4e3db",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "**Additional reading/watching**\n",
    "\n",
    "- The model we are using here is called Pix2Pix. The paper can be found here: **INSERT LINK HERE**\n",
    "\n",
    "- The implementation of the Pix2Pix model is based on a tutorial of tensorflow. You can have a look for more specific explanations here:  https://www.tensorflow.org/tutorials/generative/pix2pix\n",
    "\n",
    "- If you like to watch some explanatory videos on this, here are some examples (youtube has many to offer): **INSERT LINKs HERE**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55004a06",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
