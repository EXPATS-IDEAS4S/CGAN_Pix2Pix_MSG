{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc0b6017",
   "metadata": {},
   "source": [
    "# Tutorial 2: Conditional Adversarial Network (CGAN)\n",
    "## Generating visible from infra-red satellite images\n",
    "\n",
    "**Authors:** Paula Bigalke, Claudia Acquistapace, Daniele Corradini\n",
    "\n",
    "**Affiliation:** Institute of Geophysics and Meteorology, University of Cologne, Germany\n",
    "\n",
    "**Contact**: paula.bigalke@uni-koeln.de, cacquist@uni-koeln.de, dcorrad1@uni-koeln.de"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dressed-irish",
   "metadata": {},
   "source": [
    "In this tutorial, you will learn how to implement and train a CGAN model with satellite images. CGANs consist of two models working against each other: the generator and the discriminator. The generator tries to create fake images. At the same time the discriminator tries to distinguish between these fake and the real images. Both want to win the battle! The generator aims at creating such realistic fake images that it can fool the discriminator into believing them to be real. And the discriminator wants to be so clever that he can always recognize the fake images. We will end up with a well trained generator of fake satellite images.\n",
    "\n",
    "But wait a second - what do we need fake satellite images for? We have plenty of real ones! The thing is, images from the visible channels are only available during the day, where the sensor measures the sunlight reflected by the surface and cloud tops. Infra-red channels, on the other hand, are producing images at every time - also during the night! What if the generator could learn to recreate the visible (VIS) images from the corresponding infra-red (IR) images? Then we could generate our own VIS images also during the night and have a more complete view of the clouds and atmosphere! This kind of task to create an unknown image from a known image is called image-to-image-translation. \n",
    "\n",
    "Let's try it!\n",
    "\n",
    "This tutorial will guide you through the different steps needed for running a CGAN model. Mainly it consists of 3 parts (notebooks):\n",
    "\n",
    "**1. Dataset preparation and pre-processing** (ex2.1_data_preparation.ipynb)\n",
    "\n",
    "**2. Setting up the model** (ex2.2_setting_up_model.ipynb)\n",
    "\n",
    "**3. Training and evaluation of the model** (ex2.3_training_network.ipynb)\n",
    "\n",
    "You will be asked to fill in some parts of the code marked with **#### TODO ####** and answer some comprehension questions. Please, provide your answer right below the respective question. For each task and question you will be given points. In total there are **30 points**. An overview is given below.\n",
    "\n",
    "Have fun!!!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48f9f7bf",
   "metadata": {},
   "source": [
    "### Setting up the Notebook\n",
    "\n",
    "**Upload material to your own drive:** Go to this link and download all the material: **INSERT LINK HERE**. Upload everything to your own drive in a designated tutorial folder. \n",
    "\n",
    "**Open the notebooks:** Then open the notebooks in Google Colab.\n",
    "\n",
    "**Access permission:** You need to give Google Colab the permission to use your Drive (a window will pop up). \n",
    "\n",
    "**Adjusting path:** Specify the path to your personal Drive folder in which you saved this tutorial.\n",
    "\n",
    "**Submission Upon Completion**: After completing the exercise, submit your work through the Google Form that will be sent soon. Additionally, you are encouraged to provide feedback about the lectures and the exercise's difficulty and your overall experience. We will share a google survey with you on this at the end of the lectures, thank you for your help and feedback.\n",
    "\n",
    "**Reporting Issues**: If you will encounter any problem during the completion of the notebook, please do not hesitate to contact us."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5414361",
   "metadata": {},
   "source": [
    "### Further reading/watching\n",
    "\n",
    "- The model we are using here is called Pix2Pix. It originates from the paper \"Image-to-Image Translation with Conditional Adversarial Networks\": https://openaccess.thecvf.com/content_cvpr_2017/papers/Isola_Image-To-Image_Translation_With_CVPR_2017_paper.pdf\n",
    "\n",
    "- The implementation of the Pix2Pix model is based on a tutorial of tensorflow. You can have a look for more specific explanations here: https://www.tensorflow.org/tutorials/generative/pix2pix\n",
    "\n",
    "- If you like to watch some explanatory videos on this, feel free to look through youtube. There is always someone explaining what you are trying to do."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97d34db7",
   "metadata": {},
   "source": [
    "--------------------------------------------------------------------------------\n",
    "\n",
    "### 2.1 Dataset preparation and pre-processing (16 / 30 points)\n",
    "\n",
    "(a) Train and test dataset (2 p)\n",
    "\n",
    "- Task a.1 (1 p): Print fraction of both training data and test data.\n",
    "- Question a.2 (1 p): What are training and test datasets used for? Why is it important to split the data?\n",
    "\n",
    "(b) Load and plot example images (4 p)\n",
    "\n",
    "- Task b.1 (1 p): Load example image pair (IR image and corresponding VIS) from training dataset.\n",
    "- Task b.2 (1 p): Print image-tensor's shape, minimum and maximum values.\n",
    "- Question b.3 (1 p): Does model have more or less trainable parameters (weights) if images were larger?\n",
    "- Task b.4 (1 p): Plot both images next to each other. \n",
    "\n",
    "(c) Normalization (3 p)\n",
    "\n",
    "- Task c.1 (1 p): Normalize the example images and print their minimum and maximum values.\n",
    "- Question c.2 (1 p): To which range are they normalized?\n",
    "- Question c.3 (1 p): Why is it important to normalize the images?\n",
    "\n",
    "(d) Data augmentation (3 p)\n",
    "\n",
    "- Task d.1 (1 p): Apply random flip on the example images and plot them again.\n",
    "- Question d.2 (1 p): How high is probability of each image pair to be flipped?\n",
    "- Question d.3 (1 p): Can you think of other data augmentation techniques?\n",
    "\n",
    "(e) Loading in the complete datasets (4 p)\n",
    "\n",
    "- Task e.1 (1 p): Load in training dataset and investigate structure by varying batch size.\n",
    "- Question e.2 (1 p): How is the dataset structured? Why is the last batch smaller?\n",
    "- Question e.3 (1 p): What is contained in each batch? \n",
    "- Question e.4 (1 p): What is the role of the batch during training?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3387e3",
   "metadata": {},
   "source": [
    "### 2.2 Setting up the model (2 / 30 points)\n",
    "\n",
    "(a) Generator (1 p)\n",
    "\n",
    "- Task a.1 (1 p): Print after each layer the shape of its output x.\n",
    "\n",
    "(b) Discriminator (1 p)\n",
    "\n",
    "- Task b.1 (1 p): Create discriminator model and print summary and overview plot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7859c89",
   "metadata": {},
   "source": [
    "### 2.3 Training and evaluation of the model (12 / 30 points)\n",
    "\n",
    "(a) Monitoring the training progress (3 p)\n",
    "- Task a.1 (1 p): For each epoch, plot test images.\n",
    "- Task a.2 (1 p): For each epoch, print losses.\n",
    "- Task a.3 (1 p): For each epoch, print runtime.\n",
    "\n",
    "(b) Training the network (4 p)\n",
    "\n",
    "- Task b.1 (4 p): Train the network.\n",
    "\n",
    "(c) Interpretation of losses (5 p)\n",
    "- Task c.1 (1 p): Plot generator losses.\n",
    "- Question c.2 (1 p): Why is test loss higher (= worse)? Is that a problem?\n",
    "- Question c.3 (1 p): At which epoch could you have stopped the training? Why?\n",
    "- Question c.4 (1 p): Do you see any overfitting? Why / Why not?\n",
    "- Question c.5 (1 p): How could you evaluate the visual quality of generated images?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876d8564",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
